{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GSNl3LJonR7g",
        "_C_aLLlEpIGu",
        "Mf5JeelAeZbq",
        "hsg9TmyMHN0C"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de datos"
      ],
      "metadata": {
        "id": "GSNl3LJonR7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el apartado anterior se ha explicado los pasos que se han realizado para normalizar las reseñas de películas y TV de Amazon. Para ello, lo primero que se ha hecho ha sido descargar los datos presentes en la página web y luego se ha ido realizando distintas modificaciones a los datos para poder prepararlos para generar un modelo de clasificación binaria. Durante el preprocesamiento, se han ido generando distintos ficheros CSV con la finalidad de tener puntos de control a los cuales poder acceder en caso de que se necesitaban. Concretamente, en el apartado anterior, una vez que se han descargado los datos se ha realizado un punto de control. Este punto de control son dos ficheros CSV los cuales contienen por un lado los datos de entrenamiento y por otro lado los datos de test. Dado que el proceso de descargar de nuevo los datos desde la página web es un proceso que lleva su tiempo, este apartado se va a comenzar desde el punto de control comentado. Destacar que los datos presentes en estos ficheros son datos en bruto y no se les ha aplicado ninguna normalización.\n",
        "\n",
        "Por ello, lo primero que se va a hacer es conectarse al google drive para poder importar los ficheros."
      ],
      "metadata": {
        "id": "MfvHQUNdnVcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XncG6GZUou_I",
        "outputId": "6fd0f1ac-7491-44fa-d4c6-6b328e4ad2b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seguidamente importo los datos de entrenamiento y de test."
      ],
      "metadata": {
        "id": "1eMsMsOYo09r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Datos de entrenamiento\n",
        "import pandas as pd\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/Práctica NPL/data_train.csv\")\n",
        "train_data = train_data.dropna(subset=['review_text'])\n",
        "print(train_data.head())\n",
        "print(\"Dimensión conjunto de entrenamiento\",train_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PD7SP10o3vv",
        "outputId": "a2da6cae-4bf7-4176-ee95-c709884d8317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         review_text  rating_note\n",
            "0  I loathe most American detective dramas. There...          5.0\n",
            "1  The storyline was great.  Looking for the last...          3.0\n",
            "2  Well, at least the first film was bearable. It...          3.0\n",
            "3  Whilst not being near the best of Crosby's fil...          5.0\n",
            "4  Best season so far. The cast has outdone thems...          5.0\n",
            "Dimensión conjunto de entrenamiento (99164, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Datos de test\n",
        "import pandas as pd\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/Práctica NPL/data_test.csv\")\n",
        "test_data = test_data.dropna(subset=['review_text'])\n",
        "print(test_data.head())\n",
        "print(\"Dimensión conjunto de test\",test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb-F5gLHo_kA",
        "outputId": "406f4ec4-d95a-4eed-b438-0722fa36cd21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                         review_text  rating_note\n",
            "0  I cannot say this strongly enough: this is one...          5.0\n",
            "1  I wonder if these guys were hoping I would be ...          1.0\n",
            "2  this movie was boring, i mean real boring, don...          1.0\n",
            "3  Was an interesting movie with good acting and ...          3.0\n",
            "4  Kids liked this movie probably better than the...          4.0\n",
            "Dimensión conjunto de test (24790, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pipeline train"
      ],
      "metadata": {
        "id": "_C_aLLlEpIGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Según la definicón actual, en el contexto del Procesamiento del Lenguaje Natural (NLP), un pipeline se refiere a una **secuencia ordenada de pasos** o procesos que **se aplican a un texto o corpus de texto** para realizar una serie de **tareas de procesamiento, análisis o extracción de información**. Estos pasos pueden incluir desde tareas básicas como tokenización y limpieza de texto hasta tareas más complejas como análisis sintáctico, extracción de entidades nombradas, clasificación de sentimientos, entre otros.\n",
        "\n",
        "Como puede apreciarse, en negrita, se ha señalado los conceptos más importantes de la definición.\n",
        "\n",
        "\n",
        "*   Secuencia ordenada de pasos: Se refiere a un proceso ordenado en el que se ejecutan una serie de pasos.\n",
        "*   Se aplican a un texto o corpus de texto: Estos pasos se deben aplicar al corpus con el que se este trabajando, en este caso, las reseñas de Amazon.\n",
        "*   Tareas de procesamiento, análisis o extracción de información: Con lo que se busca con el pipeline es realizar el preprocesamiento y análisis del corpus.\n",
        "\n",
        "Por lo tanto, por medio del pipeline lo que se pretende es conseguir realizar todo el preprocesamiento de datos descrito en el apartado 1 de la práctica de forma ordenada y paso por paso, con el fin de conseguir el mismo resultado. Para ello, lo primero que se debe pensar es cuales son los pasos que se han realiado en el apartado anterior.\n",
        "\n"
      ],
      "metadata": {
        "id": "9bIzUEEDpSC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el apartado anterior, lo primero que se ha hecho ha sido explorar la distribución de los datos. Sin embargo, la conclusión de esta exploración ha sido que los datos no estaban distribuidos equitativamente pero que no se iba aplicar ninguna técnica para cambiar esta distribución debido a que se pretendía asemejar este problema a un problema real. Por lo tanto, dado que al final no se ha hecho nada, en el pipeline no es necesario incluir esta sección.\n",
        "\n",
        "A continuación, se ha procedido a realizar la normalización del corpus. Los cambios que se han realizado sobre el corpus han sido:\n",
        "\n",
        "*   Contracciones: Previo a tokenizar las palabras, primero se han expandido las contracciones dado que las reseñas están escritas en inglés.\n",
        "*   Tokenización: Se ha divido cada reseña en tokens. El resultado se ha almacenado en la columna review_tokens.\n",
        "*   Eliminación de stopwords, signos de puntuación y mayúsculas: El resultado se ha almacenado en la columna clean_tokens.\n",
        "*   Comprobación de símbolos y eliminación de los símbolos [\"``\", \"''\", \"...\", \"--\", \"..\", \"...\"]: Dado que eran tokens que se repetían mucho y que no aportaban información, se han eliminado. El resultado se ha almacenado en la columna clean_tokens.\n",
        "*   Lematización del vocabulario: Se han lematizado todos los tokens que eran verbos y se han almacenados en la columna lemmatized_tokens.\n",
        "\n",
        "Una vez que se había conseguido normalizar las reseñas, se ha procedido a analizar la cardinalidad del vocabulario y de cada reseña. Se ha realizado un análisis y como consecuencia de este estudio, se han eliminado todas aquellas reseñas que poseían más de 400 tokens ya normalizados debido a que se han considerado outliers.\n",
        "\n",
        "Seguidamente, se ha realizado un estudio sobre los datos con los que se trabaja y para ello se han analizado los unigramas, bigramas, trigramas y nube de palabras. Destacar que simplemente se ha analizado y no se han modificado los datos.\n",
        "\n",
        "Por último, se han generado los embeddings para cada reseña y se han almacenado todos ellos en un tensor resultante de dimensión (98088, 62, 100), siendo 62 el valor del tercer cuartil de los datos y 100 el número de características para cada palabra. Con estos embeddings, más adelante, se han graficado las palabras más similares a una dada.\n",
        "\n",
        "Por lo tanto, este pipeline tiene que realizar los mismos pasos que se han realiado en el apartado anterior pero solamente aquellos en los que se han modificado los datos. La entrada al pipeline tienen que ser los datos en bruto y la salida tiene que ser el tensor de embeddings. Todos los análisis, gráficas, n-grams, nube de palabras no se deben realizar. Por ello, a continuación procedo a su implementación."
      ],
      "metadata": {
        "id": "Se0dwAjCqxtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAa9ssO5pJNq",
        "outputId": "733a6eb9-690f-4fc7-be0b-a499eb2e6f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Descargo los recursos necesarios de NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')  # Nuevo\n",
        "\n",
        "# Inicializo lematizador\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Símbolos a eliminar\n",
        "symbols_to_remove = [\"``\", \"''\", \"...\", \"--\", \"..\", \"...\"]\n",
        "\n",
        "# Conjunto de stopwords y puntuación\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "# Función para expandir contracciones y quitar posesiones\n",
        "def clean_text(text):\n",
        "    expanded_text = contractions.fix(text)\n",
        "    text_without_possession = expanded_text.replace(\"'s\", \"\")\n",
        "    return text_without_possession\n",
        "\n",
        "# Función para limpiar los tokens\n",
        "def clean_tokens(tokens):\n",
        "    clean_tokens = [token.lower() for token in tokens if token.lower() not in stop_words and token not in punctuation]\n",
        "    return clean_tokens\n",
        "\n",
        "# Función para eliminar símbolos\n",
        "def remove_symbols(tokens):\n",
        "    clean_tokens = [re.sub('|'.join(map(re.escape, symbols_to_remove)), '', token) for token in tokens]\n",
        "    clean_tokens = [token.strip() for token in clean_tokens if token.strip()]\n",
        "    return clean_tokens\n",
        "\n",
        "# Función para lemmatizar los tokens\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatized_tokens = []\n",
        "    for word in tokens:\n",
        "        pos_tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "        wordnet_pos = {\n",
        "            'N': wordnet.NOUN,\n",
        "            'V': wordnet.VERB,\n",
        "            'J': wordnet.ADJ,\n",
        "            'R': wordnet.ADV\n",
        "        }.get(pos_tag, wordnet.NOUN)\n",
        "        lemmatized_word = lemmatizer.lemmatize(word, pos=wordnet_pos).lower().strip()\n",
        "        lemmatized_tokens.append(lemmatized_word)\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Crear pipeline\n",
        "def nlp_pipeline(text):\n",
        "    # Expandir contracciones y quitar posesiones\n",
        "    expanded_text = clean_text(text)\n",
        "\n",
        "    # Tokenizar el texto\n",
        "    tokens = word_tokenize(expanded_text)\n",
        "\n",
        "    # Limpiar los tokens\n",
        "    cleaned_tokens = clean_tokens(tokens)\n",
        "\n",
        "    # Eliminar símbolos\n",
        "    symbols_removed = remove_symbols(cleaned_tokens)\n",
        "\n",
        "    # Lematizar los tokens\n",
        "    lemmatized_tokens = lemmatize_tokens(symbols_removed)\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Aplico el pipeline, elimino la columna review_text y reordeno el dataser\n",
        "train_data['tokens'] = train_data['review_text'].apply(nlp_pipeline)\n",
        "train_data['cardinality'] = train_data['tokens'].apply(len)\n",
        "train_data.drop(columns=['review_text'], inplace=True)\n",
        "train_data = train_data[['tokens', 'cardinality', 'rating_note']]\n",
        "\n",
        "# Por último elimino las filas donde review_cardinality sea superior a 400\n",
        "train_data = train_data[train_data['cardinality'] <= 400]\n",
        "\n",
        "# Imprimio la dimensión del dataset para comprobar que todo se ha aplicado correctamente\n",
        "print(\"Dimensión del dataset:\", train_data.shape)\n",
        "print(train_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PX5ftP4vwBa1",
        "outputId": "593fb13f-343b-47d4-db4e-0e5f298e2c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensión del dataset: (98088, 3)\n",
            "                                              tokens  cardinality  rating_note\n",
            "0  [loathe, american, detective, drama, ending, a...          133          5.0\n",
            "1  [storyline, great, look, last, chapter, love, ...           68          3.0\n",
            "2  [well, least, first, film, bearable, glorious,...           24          3.0\n",
            "3  [whilst, near, best, crosby, film, welcome, se...           29          5.0\n",
            "4  [best, season, far, cast, outdone, kathy, bate...           12          5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el primer apartado una vez que había normalizado los datos, he procedido a generar los embeddings. Sin embargo, en este caso no voy a generarlos dado que en el apartado 3 se solicita generar dos modelos. En mi caso, voy a utilizar un modelo SVM y un modelo LSTM. Dado que el primer modelo no nececesita embeddings, no creo que sea necesario crear los embeddings en este apartado, sino que creo que es más conveniente crearlos en el momento de creación del modelo LSTM. Por ello, lo que voy a hacer es guardar el resultado obtenido en un fichero CSV para poder importarlo en el apartado 3."
      ],
      "metadata": {
        "id": "kp7NHw192MYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ruta donde guardar el archivo CSV\n",
        "ruta_archivo = \"/content/drive/MyDrive/Práctica NPL/data_train_pipeline.csv\"\n",
        "\n",
        "# Guardo el DataFrame en un archivo CSV\n",
        "train_data.to_csv(ruta_archivo, index=False, sep=';')\n"
      ],
      "metadata": {
        "id": "z4jCZ_aafbg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pipeline test"
      ],
      "metadata": {
        "id": "Mf5JeelAeZbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por otro lado, la normalización se ha realizado solamente para los datos de train. Es importante aplicar los mismos cambios a test para que así se pueda generar los modelos y que estos puedan aprender. Lo bueno de haber definido el pipeline es que es muy sencillo aplicar los cambios ya que simplemente es ejecutarlo pero en este caso a test."
      ],
      "metadata": {
        "id": "JT0JiYb7edQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Descargo los recursos necesarios de NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')  # Nuevo\n",
        "\n",
        "# Inicializo lematizador\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Símbolos a eliminar\n",
        "symbols_to_remove = [\"``\", \"''\", \"...\", \"--\", \"..\", \"...\"]\n",
        "\n",
        "# Conjunto de stopwords y puntuación\n",
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = set(string.punctuation)\n",
        "\n",
        "# Función para expandir contracciones y quitar posesiones\n",
        "def clean_text(text):\n",
        "    expanded_text = contractions.fix(text)\n",
        "    text_without_possession = expanded_text.replace(\"'s\", \"\")\n",
        "    return text_without_possession\n",
        "\n",
        "# Función para limpiar los tokens\n",
        "def clean_tokens(tokens):\n",
        "    clean_tokens = [token.lower() for token in tokens if token.lower() not in stop_words and token not in punctuation]\n",
        "    return clean_tokens\n",
        "\n",
        "# Función para eliminar símbolos\n",
        "def remove_symbols(tokens):\n",
        "    clean_tokens = [re.sub('|'.join(map(re.escape, symbols_to_remove)), '', token) for token in tokens]\n",
        "    clean_tokens = [token.strip() for token in clean_tokens if token.strip()]\n",
        "    return clean_tokens\n",
        "\n",
        "# Función para lemmatizar los tokens\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatized_tokens = []\n",
        "    for word in tokens:\n",
        "        pos_tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "        wordnet_pos = {\n",
        "            'N': wordnet.NOUN,\n",
        "            'V': wordnet.VERB,\n",
        "            'J': wordnet.ADJ,\n",
        "            'R': wordnet.ADV\n",
        "        }.get(pos_tag, wordnet.NOUN)\n",
        "        lemmatized_word = lemmatizer.lemmatize(word, pos=wordnet_pos).lower().strip()\n",
        "        lemmatized_tokens.append(lemmatized_word)\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Crear pipeline\n",
        "def nlp_pipeline(text):\n",
        "    # Expandir contracciones y quitar posesiones\n",
        "    expanded_text = clean_text(text)\n",
        "\n",
        "    # Tokenizar el texto\n",
        "    tokens = word_tokenize(expanded_text)\n",
        "\n",
        "    # Limpiar los tokens\n",
        "    cleaned_tokens = clean_tokens(tokens)\n",
        "\n",
        "    # Eliminar símbolos\n",
        "    symbols_removed = remove_symbols(cleaned_tokens)\n",
        "\n",
        "    # Lematizar los tokens\n",
        "    lemmatized_tokens = lemmatize_tokens(symbols_removed)\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Aplico el pipeline, elimino la columna review_text y reordeno el dataser\n",
        "test_data['tokens'] = test_data['review_text'].apply(nlp_pipeline)\n",
        "test_data['cardinality'] = test_data['tokens'].apply(len)\n",
        "test_data.drop(columns=['review_text'], inplace=True)\n",
        "test_data = test_data[['tokens', 'cardinality', 'rating_note']]\n",
        "\n",
        "# Por último elimino las filas donde review_cardinality sea superior a 400\n",
        "test_data = test_data[test_data['cardinality'] <= 400] #IMPORTANTE: EL umbral para considerar outliers debe ser el mimso que el de train, sino se estaría cometiendo contaminación.\n",
        "\n",
        "# Imprimio la dimensión del dataset para comprobar que todo se ha aplicado correctamente\n",
        "print(\"Dimensión del dataset:\", test_data.shape)\n",
        "print(test_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhlC2RtqeT3_",
        "outputId": "948c42f6-b1fe-48d7-9973-e7e9b1b60b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensión del dataset: (24543, 3)\n",
            "                                              tokens  cardinality  rating_note\n",
            "0  [say, strongly, enough, one, man, personal, vi...           21          5.0\n",
            "1  [wonder, guy, hop, would, influence, something...           98          1.0\n",
            "2  [movie, boring, mean, real, boring, rent, wish...           17          1.0\n",
            "3  [interest, movie, good, act, reasonable, plot,...           14          3.0\n",
            "4  [kid, like, movie, probably, well, first, one,...           14          4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardo los datos de test ya normalizados."
      ],
      "metadata": {
        "id": "0XWC_trYtQkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ruta donde guardar el archivo CSV\n",
        "ruta_archivo = \"/content/drive/MyDrive/Práctica NPL/data_test_pipeline.csv\"\n",
        "\n",
        "# Guardo el DataFrame en un archivo CSV\n",
        "test_data.to_csv(ruta_archivo, index=False, sep=';')\n"
      ],
      "metadata": {
        "id": "9U2jQmysftB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusión apartado 2"
      ],
      "metadata": {
        "id": "hsg9TmyMHN0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Al igual que en el apartado 1, recapitulo en esta sección que es lo que se ha hecho para no perder el contexto de la práctica. En este apartado lo que se ha realizado es una función que permite procesar las reseñas y prepararlas para que un modelo pueda ser entrenado. Para ello, cada reseña se ha normalizado y tokenizado en base a los mismos pasos indicados en el apartado 1 y como conseuencia se han generados dos datasets distintos: uno para train y otro para test. Ambos dos, se han almacenado en ficheros csv para que luego puedan ser utilizados en el siguiente apartado."
      ],
      "metadata": {
        "id": "1kBPn4jKsVcF"
      }
    }
  ]
}